<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[ServiceMesh简介]]></title>
      <url>%2F2019%2F02%2F26%2FServiceMesh%E7%AE%80%E4%BB%8B%2F</url>
      <content type="text"></content>
    </entry>

    
    <entry>
      <title><![CDATA[ceph官网导读]]></title>
      <url>%2F2019%2F02%2F12%2Fceph%E5%AE%98%E7%BD%91%E5%AF%BC%E8%AF%BB%2F</url>
      <content type="text"><![CDATA[ceph有的中文网站 但是更新速度比较缓慢，当前还停留在Jewel版本，所以建议还是看官网比较好。在讲官网之前，先聊一下什么是ceph，它的架构是什么样的。 什么是ceph摘抄一段wiki上的翻译：ceph是一个免费的软件存储平台，在一个分布式集群上实现存储，提供对象存储，块存储，文件系统三种类型的接口。ceph旨在提供无单点故障的分布式系统，并且可以扩展到 exabyte级，而且免费。 其实这个定义把ceph的几个重要特性都说了出来，首先是它基于一套底层结构支持对象存储，块存储和文件系统；其次它不存在单点问题，也不存在性能瓶颈，关于这一点后面会讲讲到；最后，也是很重要的一点，它是免费的！ 下面我们看一下ceph的版本，截止当前（2019.02.12）ceph的最新版本是13.2.4，发布日期是2019.01。其实从版本的更新时间和频率也可以侧面看出一个技术的活跃程度。最后再给出ceph社区的地址 中文 , 英文。 架构这里只盗用一张官网的图，具体的架构官网上很清晰中文和英文版本，这两个版本我粗略看起来差不多，如果感觉英文能力没问题，建议读英文版的，毕竟比较新。 从上图可以看出，无论是对象存储，块存储还是文件系统其实都基于底层的RADOS实现的，ceph基于RADOS提供了一个可以无限扩展的存储集群，这个集群主要有两种类型的后台进程Monitor和OSD。随后文档中讲解了数据如何存储。随后讲了如何支持扩展和高可用，引出CRUSH算法，这个是ceph的核心概念之一，就是因为有了它才实现了ceph的去中心化。 讲完基础架构之后，开始讲集群的管理引入了pool、PG等概念，并且简单描述了数据读取的流程，并且讲到了数据如何保持一致性，如何reblance等。随后用很长的篇幅讲解了ERASURE CODING它是一种和replica一样的保证数据高可用的方法。 随后还讲到了如何在ceph中增加cache层和如何自己扩展ceph。在该文章的末尾，讲解了一个ceph中很重要的概念条带化，大家对条带化这个词肯定不陌生，想要了解ceph如何利用它来增加性能的可以看一下。 官网导读看完架构之后，估计大家对ceph就有一个大致的了解了，那么就可以看一下官网了 – 地址 上图是ceph官方文档首页截图，左边是文档的菜单栏，内容如下： Intro to Ceph：介绍的组件，并且对硬件，操作系统做了一些建议；还提供给大家贡献ceph社区和贡献ceph文档的方法。 Installation，一共3个主菜单，分别告诉大家如何使用 ceph-deploy部署，如何手动部署以及如何使用k8s+helm部署。 CEPH STORAGE CLUSTER：ceph存储的基础，基于RADOS，这部分内容无论是使用对象存储，块存储还是文件系统的都建议看一下。它的子目录主要分为配置、部署、操作、帮助手册、troubleshooting、api。其中配置，操作建议认真看一下。 下面三个菜单就是ceph文件系统，ceph块存储，ceph对象存储的内容了，关于这部分，需要用到啥看啥就行。当然这三块也是整个官方文档的重点。 Ceph Manager Daemon：ceph-mgr，monitor之外的另一个对ceph集群监控的组件，可以了解。 API Documentation所有你需要的api都在这。 Architecture就是文章开始说的架构，建议最先看，先对ceph有个全方位的了解。 Developer Guide:开发者指南，把别的都了解了再看这个吧。 CEPH INTERNALS：看名字就知道，高段位再看，先把前面的都了解了吧。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[闲谈java日志]]></title>
      <url>%2F2018%2F11%2F28%2Fjava%E6%97%A5%E5%BF%97%E8%A7%84%E8%8C%83%2F</url>
      <content type="text"><![CDATA[闲谈java日志本文有如下三个主题 工具上的规范 日志规范 扩展 关于扩展的内容，先讲一个概念，大家有兴趣的可以告诉我，后面可以以专题的形式讲一下。 工具这个可以参考阿里java规范：为什么阿里巴巴禁止工程师直接使用日志系统(Log4j、Logback)中的 API 其实本质原因是，打日志的工具有很多，常用的有 java.util.logging：这个是jdk自带的 Log4j Logback log4j2 其中后面三个是同一个作者写的，这里我们暂且不比较它们的优劣，主要关注怎么用，其实开始已经说了，不要直接使用任何一个工具的api，为什么呢？主要是它们的api都是不同的，如果你的应用和某一个api紧耦合，当需要改变日志工具的时候就会非常痛苦，那怎么办呢？在计算计科学中，没有中间层解决不了的问题，如果有，那就再来一层，这时就引入了SLF4J，它就是一个中间层，它不负责实际打印日志，它只是一个Facade，也就是设计模式中常说的门面模式。 所以第一个要求就是，打日志统一使用： private static final Logger logger = LoggerFactory.getLogger(XXX.class); 关于日志工具的比价，日志门面的比较，后面可以搞专题讲。不是本篇文章的重点。 书写规范这个是本次的重点，主要侧重于log的注意事项和一些最佳实践 1. 正确的理解日志级别当前应用中有很多人对日志级别的使用不正确，或者说全部都使用一个日志级别，这个问题比较大，后面按照如下规则来区分级别： Debug : 主要供开发使用，包含的数据一般是响应时间，健康检查等。比如:”XXX response time 10 ms” Info : 业务流程，要保证QA能看懂你的日志，帮助大家理解系统。比如:”用户:张三 创建账户成功” Warning : 这些日志标识出现了异常情况，的那还是并不代表错误，而且也不需要立刻得到处理。比如:”用户输入了错误的字符：‘张三$’，忽略它” Error : 出现错误，需立即解决，这个大家估计比较熟悉 2. 日志的受众不仅仅是自己一定要记住，日志不仅仅是自己看，还要给其他程序员，QA来看，而且程序员还可能使用grep，awk等来查看日志，所以它们必须简单并且内容充实。有人建议打印两种日志，一种给机器看，一种给人看，比如： transaction was completed successfully” + transactionID “total time for transaction =” + TimeElapsed “success” + transactionID “time” + TimeElapsed 这里我们推荐使用第一种 3. 记录下与外部系统的交互系统间的问题是最难定位的，所以要求在系统间交互的地方一定要打印日志。这可能会带来一定的性能上的损耗，所以日常可以不开启，当系统出现问题的时候再开启。 4. 正确的记录异常信息记录异常是记录日志中很重要的一个环节。很多时候我们习惯记录一个异常，然后把它包装到一个自定义异常后再抛出它，这会导致异常被打印两次，这里要求不要多次打印该异常，直接把它包装到自定义异常抛出即可，在全局异常处理的地方打印堆栈信息。 BAD:1234567891011121314try &#123; Integer x = null; ++x;&#125; catch (Exception e) &#123;log.error(“IO exception”, e);throw new MyCustomException(e);&#125; GOOD: 12345678910111213try &#123; Integer x = null; ++x;&#125; catch (Exception e) &#123;throw new MyCustomException(e);&#125; 最后，用如下形式打印异常：log.error(“Error XXX {} ”, e); 以下摘抄阿里巴巴开发规范日志篇 【强制】应用中不可直接使用日志系统(Log4j、Logback)中的 API,而应依赖使用日志框架SLF4J 中的 API,使用门面模式的日志框架,有利于维护和各个类的日志处理方式统一。import org.slf4j.Logger;import org.slf4j.LoggerFactory;private static final Logger logger = LoggerFactory.getLogger(Abc.class); 【强制】日志文件推荐至少保存 15 天,因为有些异常具备以“周”为频次发生的特点。 【强制】应用中的扩展日志(如打点、临时监控、访问日志等)命名方式: appName_logType_logName.log。logType:日志类型,推荐分类有 stats/desc/monitor/visit 等;logName:日志描述。这种命名的好处:通过文件名就可知 道日志文件属于什么应用,什么类型,什么目的,也有利于归类查找。——禁止用于商业用途,违者必究—— 19 /35阿里巴巴 Java 开发手册 正例:mppserver 应用中单独监控时区转换异常,如:mppserver_monitor_timeZoneConvert.log说明:推荐对日志进行分类,如将错误日志和业务日志分开存放,便于开发人员查看,也便于 通过日志对系统进行及时监控。 【强制】对 trace/debug/info 级别的日志输出,必须使用条件输出形式或者使用占位符的方 式。说明:logger.debug(“Processing trade with id: “ + id + “ and symbol: “ + symbol); 如果日志级别是 warn,上述日志不会打印,但是会执行字符串拼接操作,如果 symbol 是对象, 会执行 toString()方法,浪费了系统资源,执行了上述操作,最终日志却没有打印。 正例:(条件)if (logger.isDebugEnabled()) {logger.debug(“Processing trade with id: “ + id + “ and symbol: “ + symbol);}正例:(占位符)logger.debug(“Processing trade with id: {} and symbol : {} “, id, symbol); 【强制】避免重复打印日志,浪费磁盘空间,务必在 log4j.xml 中设置 additivity=false。 正例: 【强制】异常信息应该包括两类信息:案发现场信息和异常堆栈信息。如果不处理,那么通过 关键字 throws 往上抛出。正例:logger.error(各类参数或者对象toString + “_” + e.getMessage(), e); 【推荐】谨慎地记录日志。生产环境禁止输出 debug 日志;有选择地输出 info 日志;如果使 用 warn 来记录刚上线时的业务行为信息,一定要注意日志输出量的问题,避免把服务器磁盘 撑爆,并记得及时删除这些观察日志。 说明:大量地输出无效日志,不利于系统性能提升,也不利于快速定位错误点。记录日志时请 思考:这些日志真的有人看吗?看到这条日志你能做什么?能不能给问题排查带来好处? 【参考】可以使用 warn 日志级别来记录用户输入参数错误的情况,避免用户投诉时,无所适 从。注意日志输出的级别,error 级别只记录系统逻辑出错、异常等重要的错误信息。如非必 要,请不要在此场景打出 error 级别。 以上说的没毛病，对于 1，2，5 已于代码模板中实现，其余各条需要大家遵守 日志追踪最后说说日志中的另一块问题，说大点就是全链路跟踪，简单点来说就是在日志中打印出traceId，使整个系统串联起来。随着业务越来越复杂，日志的追踪也变得越来越难，一个web服务可能经历很多流转，所以把日志串联起来就显得至关重要，其实全链路监控也是依托这个实现的。 先看一些分布式追踪系统的概念，如果英语好的同学可以看一下Google的论文Dapper 上图就是一个日志追踪系统（zipkin）的直观视图，里面有如下几个重要概念： span: 工作基本单元，一个traces过程中会有很多个span。它一般被两个64bit的ID组合而成，前一个标识该span在哪个trace中，后一个标识该span。（初始的span被称为root span,它的ID和trace的ID相同） trace: 一系列树状的span annotation: 用于记录当前事件，比如在Zipkin中会用它标识哪些是client，哪些是server，请是从哪开始的，从哪里结束的。 cs: client sent sr: server received ss: server sent cr: client received 上图中每个颜色标明一个span（A to G） spring使用sleuth+zipkin来实现这种追踪监控，功能强大，扩展性也比较强。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java11新特性--Epsilon:A No-Op Garbage Collector]]></title>
      <url>%2F2018%2F10%2F03%2Fjava11%E6%96%B0%E7%89%B9%E6%80%A7-A-No-Op-Garbage-Collector%2F</url>
      <content type="text"><![CDATA[简介Epsilon垃圾回收器控制内存分配，但是不执行任何垃圾回收工作。一旦java的堆被耗尽，jvm就直接关闭。 目标提供一个完全消极的GC实现，分配有限的内存分配，最大限度降低消费内存占用量和内存吞吐时的延迟时间。一个好的实现是隔离代码变化，不影响其他GC，对小限度的改变其他的JVM代码。 误区它不是为了提供给java语言或jvm一个手动管理内存的功能。它不是为了介绍一种新的管理java heap的API。它不是为了适配自己而改变或清除JVM的内部接口。 动机java实现了一系列的高度可配置化的GC实现。各种不同的垃圾回收器可以面对各种情况。但是有些时候使用一种独特的实现，而不是将其堆积在其他GC实现上将会是事情变得更加简单。 下面是no-op GC的几个使用场景： Performance testing,什么都不执行的GC非常适合用于差异性分析。no-op GC可以用于过滤掉GC诱发的新能损耗，比如GC线程的调度，GC屏障的消耗，GC周期的不合适触发，内存位置变化等。此外有些延迟者不是由于GC引起的，比如scheduling hiccups, compiler transition hiccups，所以去除GC引发的延迟有助于统计这些延迟。 Memory pressure testing, 在测试java代码时，确定分配内存的阈值有助于设置内存压力常量值。这时no-op就很有用，它可以简单地接受一个分配的内存分配上限，当内存超限时就失败。例如：测试需要分配小于1G的内存，就使用-Xmx1g参数来配置no-op GC，然后当内存耗尽的时候就直接crash。 VM interface testing, 以VM开发视角，有一个简单的GC实现，有助于理解VM-GC的最小接口实现。它也用于证明VM-GC接口的健全性。 Extremely short lived jobs, 一个短声明周期的工作可能会依赖快速退出来释放资源，这个时候接收GC周期来清理heap其实是在浪费时间，因为heap会在退出时清理。并且GC周期可能会占用一会时间，因为它依赖heap上的数据量。 Last-drop latency improvements, 对那些极端延迟敏感的应用，开发者十分清楚内存占用，或者是几乎没有垃圾回收的应用，此时耗时较长的GC周期将会是一件坏事。 Last-drop throughput improvements, 即便对那些无需内存分配的工作，选择一个GC意味着选择了一系列的GC屏障，所有的OpenJDK GC都是分代的，所以他们至少会有一个写屏障。避免这些屏障可以带来一点点的吞吐量提升。 描述Epsilon垃圾回收器和其他OpenJDK的垃圾回收器一样，使用-XX:+UseEpsilonGC开启。 Epsilon线性分配单个连续内存块。可复用现存VM代码中的TLAB部分的分配功能。非TLAB分配也是同一段代码，因为在此方案中，分配TLAB和分配大对象只有一点点的不同。 Epsilon用到的barrier是空的(或者说是无操作的)。因为该GC不执行任何的GC周期，不用关系对象图，对象标记，对象复制等。引进一种新的barrier-set实现可能是该GC对JVM最大的变化。 当java heap耗尽的时候，因为没有内存回收，没有内存再利用，所以我们会得到系统出错。下面有几个选项，这些大多在现有GC中已经出现过了： 抛出带有描述信息的OutOfMemoryError异常 执行heap dump操作（当带有-XX:+HeapDumpOnOutOfMemoryError时） 失败掉JVM并且执行可选的外部操作(使用 -XX:OnOutOfMemoryError=…)，比如开启一段调试程序或者通知外部监控系统 System.gc()将变得无效，因为没有内存回收操作。它的实现可能是告诉用户尝试强行GC是无效的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java11新特性--Local-Variable Syntax for Lambda Parameters]]></title>
      <url>%2F2018%2F09%2F29%2Fjava11%E6%96%B0%E7%89%B9%E6%80%A7-Local-Variable-Syntax-for-Lambda-Parameters%2F</url>
      <content type="text"><![CDATA[简介允许在隐式类型的lambda表达式中使用var来声明参数 目标统一隐式类型lambda表达式参数声明语法和普通本地变量声明语法 动机一个lambda表达式可能是隐式类型的，它的类型是靠推断获得的，如下：1(x, y) -&gt; x.process(y) 在java 10中出现了隐式类型的本地变量：123var x = new Foo();for (var x : xs) &#123; ... &#125;try (var x = ...) &#123; ... &#125; catch ... 为了统一本地变量，我们希望能够在隐式的lambda表达式中使用var声明参数：1(var x, var y) -&gt; x.process(y) 这种统一的一个好处就是modifiers（特别是一些注解）可以完全一致的用于本地变量和lambda表达式: 12@Nonnull var x = new Foo();(@Nonnull var x, @Nullable var y) -&gt; x.process(y) 描述对于隐式类型的lambda表达式，允许省略掉var，如下： 12@Nonnull var x = new Foo();(@Nonnull var x, @Nullable var y) -&gt; x.process(y) 和下面是等价的 1(x, y) -&gt; x.process(y) 隐含类型的lambda表达式的参数要么全部使用var，要么全不使用var。并且var只能用于隐含类型的lambda表达式，那些显示类型的lambda表达式是不能用var的，比如下面的例子是不合法的： 12(var x, y) -&gt; x.process(y) (var x, int y) -&gt; x.process(y) 理论上说上面的第二种半隐式类型（也有人叫半显示类型），但是它已经超出本JEP的讨论范围了，因为它影响到推断和重载机制。这就是为什么当前强制限制lambda表达式只能是全显示或隐式的原因。我们也强制无论隐式lambda表达式的参数是否带var都不影响推断。我们可能在未来的JEP中解决部分推断的问题，但是我们不希望你以如下速记方式的语法，所以下面的表达式是不合法的： 1var x -&gt; x.foo()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java11新特性---Nest-Based Access Control(嵌套访问控制)]]></title>
      <url>%2F2018%2F09%2F27%2Fjava11%E6%96%B0%E7%89%B9%E6%80%A7-Nest-Based-Access-Control%2F</url>
      <content type="text"><![CDATA[简介嵌套是一种访问控制上下文，它允许多个class同属一个逻辑代码块，但是被编译成多个分散的class文件，它们访问彼此的私有成员无需通过编译器添加访问扩展方法。 动机很多jvm语言支持在一个源文件中放多个class。这对于用户是透明的，用户认为它们在一个class中，所以希望它们共享同一套访问控制体系。为了达到目的，编译器需要经常需要通过附加的access bridge扩大private成员的访问权限到package。这种bridge和封装相违背，并且轻微的增加程序的大小，会干扰用户和工具。所以我们希望一种更直接，更安全，更透明的方式。 一个更大的坑就是反射的时候会有问题。当使用java.lang.reflect.Method.invoke从一个nestmate调用另一个nestmate私有方法时会报IllegalAccessError错误。这个是让人不能理解的，因为反射应该和源码级访问拥有相同权限。 话不多说，看段代码1234567891011121314151617181920212223public class JEP181 &#123; public static class Nest1 &#123; private int varNest1; public void f() throws Exception &#123; final Nest2 nest2 = new Nest2(); //这里没问题 nest2.varNest2 = 2; final Field f2 = Nest2.class.getDeclaredField("varNest2"); //这里在java8环境下会报错，在java11中是没问题的 f2.setInt(nest2, 2); System.out.println(nest2.varNest2); &#125; &#125; public static class Nest2 &#123; private int varNest2; &#125; public static void main(String[] args) throws Exception &#123; new Nest1().f(); &#125;&#125; 在java11之前，classfile用InnerClasses和EnclosingMethod两种属性来帮助编译器确认源码的嵌套关系，每一个嵌套的类型会编译到自己的class文件中，在使用上述属性来连接其他class文件。这些属性对于jvm确定嵌套关系上已经足够了，但是它们不直接适用于访问控制，并且和java语言绑定的太紧了。 为了提供一种更大的，更广泛的，不仅仅是java语言的嵌套类型，并且补足访问控制检测的不足，引入了两个新的class文件属性。定义了两种nest member，一种叫nest host（也叫top-level class），它包含一个NestMembers属性用于确定其他静态的nest members，其他的就是nest member，它包含一个NestHost属性用于确定它的nest host。 大家可以看一下上述代码的class文件详情。 JVM针对嵌套成员的访问控制调整了jvm访问规则，增加了如下条款：一个field或method R可以被class或interface D访问，当且仅当如下任一条件为真： … …(原条款不变) R是私有的，并且声明在另一个class或interface C中，并且C和D是nestmates C和D是nestmates表名他们肯定有一个相同的host 这个松散的访问规则会作用在如下几个地方：（这一段我就贴原文了，感觉翻译过来味道就变了） Resolving fields and methods (JVMS 5.4.3.2, etc.) Resolving method handle constants (JVMS 5.4.3.5) Resolving call site specifiers (JVMS 5.4.3.6) Checking Java language access by instances of java.lang.reflect.AccessibleObject Checking access during queries to java.lang.invoke.MethodHandles.Lookup 针对上述访问规则的改变，相应的调整字节码： invokespecial for private nestmate constructors, invokevirtual for private non-interface, nestmate instance methods, invokeinterface for private interface, nestmate instance methods; and invokestatic for private nestmate, static methods 嵌套类的校验嵌套类必须在访问前校验。校验最迟要发生在访问成员之前，最早可以发生在对class文件的校验时，或者在两者之间，比如JIT时。校验嵌套关系，需要加载nest host类，为了防止无意义的加载，这一步尽量放到最后做。 为了保证嵌套的完整性，建议禁止修改nest classfile属性]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Git 分支管理规范]]></title>
      <url>%2F2018%2F09%2F12%2FGit-%E5%88%86%E6%94%AF%E7%AE%A1%E7%90%86%E8%A7%84%E8%8C%83%2F</url>
      <content type="text"><![CDATA[本文不涉及git指令的介绍，侧重于分支管理，个人理解分支管理并没有正确与否，重要在于是否成体系，简单来说就是能否高效的应对所有情况。没有对比就没有伤害，先看两种比较出名的分支管理方式: Git flow原文链接根红苗正，很多注重流程的大公司都在使用。先看一张总览图：大家可以仔细看一下这幅图，带着问题继续向下看 概念在上图中有如下几个概念： 时间线：这个就不多说了，牵扯到一些哲学问题 master：生产分支，和生产版本一一对应 develop分支：日常开发所在分支 feature分支：功能开发所在分支 release分支：预发版本 hotfixes分支：修复线上bug的分支（哎。。希望大家都用不到。。） master &amp; develop这是最核心的两个分支，从图上可以看出，他们的时间线是两根实现，意味着这两个分支是永存的。 首先master分支，大家应该很熟悉，在这里它始终代表着生产环境的状态。 develop分支，在它上面最新提交的代码始终代表着下一个release的改动。当develop分支的代码达到一种可以release的稳定状态时，所有的改变都应该以某种方式合并会master上，并且打上release的tag。后面会聊如何合并回master。(见release的介绍) 辅助分支在两个核心分支之外，我们还有几个辅助分支，用于支撑多团队开发，新功能开发，准备生产release，快速修复生产bug等问题。这些分支的生命周期是有限的，最终都会被删除。 这些分支是： feature分支 release分支 hotfix分支 上述的每个分支都有特殊的目的，并且有严格的规范，比如它们源于哪个分支，最终必须合并到哪个分支，下面我们就来讨论这个。 注：这里的辅助分支从技术层面上来看并无任何特殊之处 feature分支源于：develop合并回：develop命名规范：除了master, develop, release-*, hotfix-*外的所有名字，建议以feature/开头feature分支用于为即将到来的或者是以后的release开发新功能。在刚开始开发feature分支时，可能并不知道它将会被合并到哪个release中。feature分支的本质是只要feature在开发，它就一直存在，直到合并到develop或者丢弃。 操作步骤 创建分支从develop分支开辟feature分支 12$ git checkout -b feature/myfeature developSwitched to a new branch &quot;feature/myfeature&quot; 把完成的feature分支合并回develop完成的feature分支将被合并回develop，以确保它能出现在即将发布的release中。 12345678$ git checkout developSwitched to branch &apos;develop&apos;$ git merge --no-ff feature/myfeatureUpdating ea1b82a..05e9557(Summary of changes)$ git branch -d feature/myfeatureDeleted branch feature/myfeature (was 05e9557).$ git push origin develop –no-ff即便在fast-forward的情况下也会增加一个commit对象，它方便记录feature的情况。下图比较了加与不加的区别： 不加–no-ff会导致你无法找到feature是哪个提交，你只能人工的读每一个提交log，然后找到那些提交。虽然加了–no-ff会增加一个空的commit对象，但是和获得的好处比就显得微不足道了。 release 分支源于：develop合并回：develop，master命名规范：release-* 用于准备新的生产发布，它也可以在最后关头做微量修改。比如很小的bug修复，生产元数据的准备等。release会在develop分支达到生产新版本预期的状态时，从develop中开辟出来。此时所有想要出现在即将发布的生产版本的feature都应该已经合并到develop，所有希望在以后的生产版本中发布的feature都不能在develop分支中。 操作步骤 创建release分支release是从develop分支开辟出来的。比如1.1.5是当前的版本，我们现在有一个大的版本即将到来，并且develop分支已经做好该版本的准备，并且我们决定给这个即将到来的版本取名为1.2版本：12345$ git checkout -b release-1.2 developSwitched to a new branch &quot;release-1.2&quot;$ git commit -a -m &quot;Bumped version number to 1.2&quot;[release-1.2 74d9424] Bumped version number to 1.21 files changed, 1 insertions(+), 1 deletions(-) 这个新的分支将会存在一段时间，知道版本发布，在这段时间中，可以在该分支上修复bug(而不是在develop分支)。但是禁止添加大的feature，新的feature必须合并到develop，然后等待下个版本的到来。 完成分支当release分支已经做好成为新版本的准备的时候，要完成如下几步，首先，releas分支要合并到master上(谨记！！！master上的每一个提交都是release)。接着，master上打上版本标签。最后，release上的改动要合并回develop，以确保后面的版本中能包含bug fix的内容。 前两步如下： 123456$ git checkout masterSwitched to branch &apos;master&apos;$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes)$ git tag -a 1.2 然后把改动merge回develop12345$ git checkout developSwitched to branch &apos;develop&apos;$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes) 这一步可能有冲突，但是没关系，合并解决它！ 最后，删除它12$ git branch -d release-1.2Deleted branch release-1.2 (was ff452fe). Hotfix分支源于：master合并回：develop/release，master命名规范：hotfix-*hotfix和release分支非常像，它们都是为新版本准备的。当线上环境出现严重的bug时，hotfix分支会从master分支开辟出来，这时在develop分支开发的人可以继续开发，负责修复bug的人可以在该分支上修复bug。 操作步骤 创建hotfix分支比如当前版本为1.2，生产遇到严重问题，但是develop尚不稳定。我们需要开辟一个hotfix分支，然后修复这个问题：12345$ git checkout -b hotfix-1.2.1 masterSwitched to a new branch &quot;hotfix-1.2.1&quot;$ git commit -a -m &quot;Bumped version number to 1.2.1&quot;[hotfix-1.2.1 41e61bb] Bumped version number to 1.2.11 files changed, 1 insertions(+), 1 deletions(-) 然后修复该问题，提交123$ git commit -m &quot;Fixed severe production problem&quot;[hotfix-1.2.1 abbe5d6] Fixed severe production problem5 files changed, 32 insertions(+), 17 deletions(-)s 完成hotfix分支当完成hotfix之后，hotfix分支需要合并到master，还需要合并到develop中，确保下个版本中会包含该hotfix，这点和release非常像。 更新master，tag版本123$ git commit -m &quot;Fixed severe production problem&quot;[hotfix-1.2.1 abbe5d6] Fixed severe production problem5 files changed, 32 insertions(+), 17 deletions(-) 合并hotfix到develop123$ git commit -m &quot;Fixed severe production problem&quot;[hotfix-1.2.1 abbe5d6] Fixed severe production problem5 files changed, 32 insertions(+), 17 deletions(-) ！！！特殊情况如果当前存在release分支，此时hotfix分支需要合并到release上，而不是develop上。这样当release合并会develop时也能保证develop上有hotfix的内容。（如果这个bug不能等待release，急需合并到develop中，也没关系，大胆的合并到develop吧~）12$ git branch -d hotfix-1.2.1Deleted branch hotfix-1.2.1 (was abbe5d6). 优缺点 优点条理清晰，环环相扣，适合注重流程管理的公司 缺点太复杂feature合并到develop分支后，如果计划有变不希望它出现在下个release中时会很麻烦多环境集成测试比较麻烦 AoneFlow阿里巴巴用的分支管理方式。从功能上来说是集大成者，灵活，简单，能满足各种需求。缺点暂时不说，我们先看一下吧。 AoneFlow只有三类分支 + 三条基本规则 分支 主干分支：master 特性分支：开发所在分支 发布分支：想要想主干发布的分支 规则1.所有特性分支由主干创建从代表最新已发布版本的主干上创建一个特性分支，一般以feature/为前缀来命名，然后在其上开始开发。 2.合并特性分支生成发布分支这个是精髓所在！！从主干上拉取一条新的分支，将所有本次要集成或发布的特性分支一次合并过去，从而得到发布分支。发布分支通常以release/为前缀。这个规则看似简单，其实可以玩出很多花样，比如我们可以将发布分支和环境对应起来，release/test对应测试环境，release/dev对应开发环境，release/prod对应线上正式环境。所有分发布分支都是动态组成的，这样带来的优点是调整起来特别方便。比如，项目就要上线了，“无敌的”甲方要求去掉某个功能，这时如果按照往常的办法，feature分支肯定已经在develop分支上了，只能手工剔除代码了。但是在AoneFlow模式下，重建发布分支就是分分钟的事情，三步走，首先删除原来的发布分支，然后从主干拉取新的同名发部分支，最后把需要保留的特性分支合并过来。这一系列操作可以很简单的自动化，并且干净无污染。 此外，发布分支之间是松耦合的，这样就可以有多个集成环境分别进行不同的特性组合的集成测试，也能方便的管理各个特性进入到不同环境上部署的时机。松耦合并不代表没有相关性，由于测试环境、集成环境、预发布环境、灰度环境和线上正式环境等发布流程通常是顺序进行的，在流程上可以要求只有通过前一环境验证的特性，才能传递到下一个环境做部署，形成漏斗形的特性发布流。 3. 发布到线上正式环境后，合并相应的发布分支到主干，在主干添加标签，同时删除该发布分支&amp;关联的特性分支当一条发布分支上的流水线完成了一次线上正式环境的部署，就意味着相应的功能真正的发布了，此时应该将这条发布分支合并到主干。为了避免在代码仓库里堆积大量历史上的特性分支，还应该清理掉已经上线部分特性分支。与 GitFlow 相似，主干分支上的最新版本始终与线上版本一致，如果要回溯历史版本，只需在主干分支上找到相应的版本标签即可。 优缺点优点： 简单，清晰，能应对各种情况缺点： 需要一套自动化工具，否则发布分支的创建，维护，重建等工作会把人干蹦 ————————— 我是分割线 ————————– 我们用什么？那么问题来了，我们应该怎么办呢？以下为个人臆想，如有不对请指出。首先，我不想用第一种方式，太繁琐，而且死板，所以大的基调定为第二种方案。但是，我们当前又没有非常好的CI/CD工具，生搬硬套难度较大，主要问题如下： 发布分支的灵活性带来了极大的复杂性，手动维护是个噩梦 无法限制分支提交顺序，比如当一个分支想要发布到线上时，很难判断它经过了dev-&gt;test-&gt;staging的验证 所以，有以下改动点： 沿用主干分支，特性分支，发布分支的概念，但是同一时刻，只有3个发布分支，分别是release/dev, release/test,release/staging分别对应着开发发布分支（用于发布到开发环境），测试发布分支（发布到测试环境），预发发布分支（发布到预发环境) 发布分支的上线步骤dev-&gt;test-&gt;staging-&gt;生产比如：上述例子中，开始我们有3个特性分支需要开发，1.0.0版本只需特性分支1上线]]></content>
    </entry>

    
  
  
</search>
